{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"logo_transfer.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","# start your code here\n","os.chdir(\"/content/drive/MyDrive/DeepLearningInPractice/homework 12\") # please change the path to the path where your homework10 folder is\n","# end your code here"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bTpg6z-oX-qr","executionInfo":{"status":"ok","timestamp":1650321532459,"user_tz":240,"elapsed":1849,"user":{"displayName":"Itunu Adedeji","userId":"17489932515323240045"}},"outputId":"b55145ab-3fe3-4b71-bac9-ced7a0033d80"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%tensorflow_version 1.x\n","import tensorflow as tf\n","print('TensorFlow version:', tf.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fjm_I-GmhHy7","executionInfo":{"status":"ok","timestamp":1650321259864,"user_tz":240,"elapsed":5,"user":{"displayName":"Itunu Adedeji","userId":"17489932515323240045"}},"outputId":"ba6dd890-af3e-4ff4-f5f2-3c929856f697"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 1.15.2\n"]}]},{"cell_type":"code","source":["pip install distance_transform"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jxtMdXX_2w6H","executionInfo":{"status":"ok","timestamp":1650322009435,"user_tz":240,"elapsed":1279,"user":{"displayName":"Itunu Adedeji","userId":"17489932515323240045"}},"outputId":"362818c2-7c82-48c4-f6a6-2a2bddac405b"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement distance_transform (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for distance_transform\u001b[0m\n"]}]},{"cell_type":"markdown","source":["Please make sure that the TensorFlow verstion is `1.x.x`. "],"metadata":{"id":"fAw8dn4R-FC9"}},{"cell_type":"code","source":["import numpy as np\n","import cv2\n","import os\n","import time\n","import argparse\n","\n","import distance_transform\n","import utility\n","import model\n","\n","###############################################################################\n","# Constants for the image input and output.\n","###############################################################################\n","\n","# Output folder for the images.\n","OUTPUT_DIR = 'output/'\n","\n","# Content image to use.\n","content_input_path = \"input/contents/\"\n","\n","#start your code here\n","content_with_ext   = \"COE.png\" # You can choose your own content image (so far, I only test jpg and png files.). please make sure that your content images are stored in the subfolder \"input/contents/\" in the homework12 folder\n","# end your code here\n","content_image_path = content_input_path + content_with_ext\n","content_image      = content_with_ext[:-4]\n","\n","# Style image to use.\n","style_input_path   = \"input/styles/\"\n","\n","#start your code here\n","style_with_ext     = \"golden_hexagon.jpg\" # You can choose your own style image (so far, I only test jpg and png files.). please make sure that your style images are stored in the subfolder 'input/styles/' in the homework12 folder\n","# end your code here\n","\n","style_image_path   = style_input_path + style_with_ext\n","style_image        = style_with_ext[:-4]\n","\n","# Invertion of images\n","content_invert = 1\n","style_invert = 1\n","result_invert = content_invert\n","###############################################################################\n","# Algorithm constants\n","###############################################################################\n","\n","# path to weights of VGG-19 model\n","VGG_MODEL = \"imagenet-vgg-verydeep-19.mat\"\n","# The mean to subtract from the input to the VGG model. \n","MEAN_VALUES = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))\n","print('success downloading VGG model')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"jIvDKaweacAz","executionInfo":{"status":"error","timestamp":1650322013148,"user_tz":240,"elapsed":19,"user":{"displayName":"Itunu Adedeji","userId":"17489932515323240045"}},"outputId":"e9217305-7234-4b90-bff0-72169ccfbacd"},"execution_count":17,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-dbcaa0739a23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdistance_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'distance_transform'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["import sys\n","sys.argv = ['']\n","\n","parser = argparse.ArgumentParser(description='A Neural Algorithm of Artistic Style')\n","parser.add_argument('--w1', '-w1',type=float, default='1',help='w1')\n","parser.add_argument('--w2', '-w2',type=float, default='1',help='w2')\n","parser.add_argument('--w3', '-w3',type=float, default='1',help='w3')\n","parser.add_argument('--w4', '-w4',type=float, default='1',help='w4')\n","parser.add_argument('--w5', '-w5',type=float, default='1',help='w5')\n","\n","parser.add_argument(\"--CONTENT_IMAGE\", \"-CONTENT_IMAGE\", type=str, default = content_image_path, help = \"Path to content image\")\n","\n","parser.add_argument(\"--IMAGE_WIDTH\", \"-IMAGE_WIDTH\",type=int, default = 400, help = \"width & height of image\")\n","\n","\n","parser.add_argument(\"--STYLE_IMAGE\", \"-STYLE_IMAGE\", type=str, default = style_image_path, help = \"Path to style image\")\n","\n","parser.add_argument(\"--alpha\",  \"-ALPHA\",type=float,  default=\"0.001\",   help=\"alpha\")\n","\n","parser.add_argument(\"--beta\",   \"-BETA\", type=float,  default=\"0.8\",     help=\"beta\")\n","parser.add_argument(\"--gamma\",  \"-GAMMA\",type=float,  default=\"0.001\",    help=\"gamma\")\n","parser.add_argument(\"--epoch\",  \"-EPOCH\",type=int, default=5000, help=\"number of epochs to run\" )\n","print('success updating defaul values')\n","args = parser.parse_args()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P0xtA-yda5zL","executionInfo":{"status":"ok","timestamp":1649708100764,"user_tz":240,"elapsed":13,"user":{"displayName":"Lichun Li","userId":"15395584703944545137"}},"outputId":"62fc08a8-fcee-45bf-8443-c5d575535f36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["success updating defaul values\n"]}]},{"cell_type":"code","source":["# Image dimensions constants. \n","# image = Image.open(content_image_path)  \n","#IMAGE_WIDTH = image.size[0]\n","IMAGE_WIDTH = args.IMAGE_WIDTH\n","IMAGE_HEIGHT = IMAGE_WIDTH\n","COLOR_CHANNELS = 3\n","\n","######### start your code here\n","\n","# Number of iterations to run.\n","ITERATIONS = 5000 # You can choose this number to be any integer you like. Currently, the default number is 5000\n","\n","# Style image layer weights\n","w1 = args.w1         # you can change the weights for the 5 layers to be a real number like 1.5.  Currently, all weights are 1. \n","w2 = args.w2\n","w3 = args.w3\n","w4 = args.w4\n","w5 = args.w5\n","# end your code here\n","\n","# Content & Style weights\n","alpha = 1e-3   # you can change the content (alpha), style (beta), and distance (gamma) weights to be any real number. currently, they are 0.001, 0.8, and 0.001. \n","beta = 1\n","gamma = 1e-2\n","\n","########### end your code here\n","\n","CONTENT_IMAGE = args.CONTENT_IMAGE\n","STYLE_IMAGE = args.STYLE_IMAGE\n","\n","# Splitting content path & name\n","dot = 0\n","slash = 0\n","for c in reversed(CONTENT_IMAGE):\n","    dot += 1\n","    if c == \".\":\n","        break\n","for c in reversed(CONTENT_IMAGE):\n","    slash += 1 \n","    if c ==\"/\" or c ==\"\\\\\":\n","        break\n","content_path = CONTENT_IMAGE[:1-slash]\n","content_name = CONTENT_IMAGE[1-slash:-dot]\n","\n","# Splitting style path & name\n","dot = 0 \n","slash = 0 \n","for c in reversed(STYLE_IMAGE):\n","    dot += 1\n","    if c == \".\":\n","        break\n","for c in reversed(STYLE_IMAGE):\n","    slash += 1\n","    if c == \"/\" or c ==\"\\\\\":\n","        break\n","style_path = STYLE_IMAGE[:1-slash]\n","style_name = STYLE_IMAGE[1-slash:-dot]\n","print(style_path,style_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"228mz5yKdmT7","executionInfo":{"status":"ok","timestamp":1649708100764,"user_tz":240,"elapsed":10,"user":{"displayName":"Lichun Li","userId":"15395584703944545137"}},"outputId":"bb01110b-38d8-4272-82e5-ece039d0be52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["input/styles/ golden_hexagon\n"]}]},{"cell_type":"code","source":["def style_loss_func(sess, model):\n","    \"\"\"\n","    Style loss function as defined in the paper.\n","    \"\"\"\n","    def gram_matrix(F, N, M):\n","        \"\"\"\n","        The gram matrix G.\n","        \"\"\"\n","        Ft = tf.reshape(F, (M, N))\n","        return tf.matmul(tf.transpose(Ft), Ft)\n","\n","    def style_loss(a, x):\n","        \"\"\"\n","        The style loss calculation.\n","        \"\"\"\n","        # N is the number of filters (at layer l).\n","        N = a.shape[3]\n","        # M is the height times the width of the feature map (at layer l).\n","        M = a.shape[1] * a.shape[2]\n","        # A is the style representation of the original image (at layer l).\n","        A = gram_matrix(a, N, M)\n","        # G is the style representation of the generated image (at layer l).\n","        G = gram_matrix(x, N, M)\n","        result = (1 / (4 * N**2 * M**2)) * tf.reduce_sum(tf.pow(G - A, 2))\n","        return result\n","\n","    # Style layers to use.\n","    layers = [\n","            ('conv1_2', w1),\n","            ('conv2_2', w2),\n","            ('conv3_2', w3),\n","            ('conv4_2', w4),\n","            ('conv5_2', w5),\n","            ]\n","\n","    E = [style_loss(sess.run(model[layer_name]), model[layer_name]) for layer_name, _ in layers]\n","    W = [w for _, w in layers]\n","    loss = sum([W[l] * E[l] for l in range(len(layers))])\n","    return loss\n","\n","def content_loss_func(sess, model):\n","    \"\"\"\n","    Content loss function as defined in the paper.\n","    \"\"\"\n","    def content_loss(p, x):\n","\n","        return 0.5 * tf.reduce_sum(tf.pow(x - p, 2))\n","    loss = content_loss(sess.run(model['conv4_2']), model['conv4_2'])\n","    return loss\n","\n","def content_dist(sess, model):\n","\n","    dist, dist_sum = distance_transform.dist_t(sess.run(model[\"input\"]))\n","    return tf.convert_to_tensor(dist, dtype=tf.float32), dist_sum\n","\n","def shape_loss_func(sess, model, dist_template, dist_sum):\n","\n","    content_image = sess.run(model['input'])\n","    mixed_image   = model[\"input\"]\n","\n","    # Convert to grayscale\n","    content_image = tf.image.rgb_to_grayscale(content_image)\n","    mixed_image   = tf.image.rgb_to_grayscale(mixed_image)\n","\n","    # Remove dimensions of size 1 from the shape of a tensor\n","    content_image = tf.squeeze(content_image)\n","    mixed_image   = tf.squeeze(mixed_image)\n","\n","    # Pixel-wise multiplication\n","    content_dist  = content_image * dist_template\n","    mixed_dist    = mixed_image   * dist_template\n","\n","    loss_tensor = 0.5 * tf.reduce_sum(tf.pow(content_dist-mixed_dist, 2))\n","\n","    return loss_tensor\n"],"metadata":{"id":"GE6LKah8eZ1T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start_time = time.time()\n","with tf.Session() as sess: \n","  # Load images.\n","  content_image = utility.load_image(CONTENT_IMAGE, IMAGE_HEIGHT, IMAGE_WIDTH, invert = content_invert)\n","  style_image   = utility.load_image(STYLE_IMAGE, IMAGE_HEIGHT, IMAGE_WIDTH, invert = style_invert)\n","  # utility.save_image(OUTPUT_DIR+\"/\"+style_name+\".png\", style_image, invert = style_invert)\n","\n","  # Load the model.\n","  model = model.load_vgg_model(VGG_MODEL, IMAGE_HEIGHT, IMAGE_WIDTH, 3)\n","  # Content image as input image\n","  initial_image = content_image.copy()\n","  # Initialize all variables\n","  sess.run(tf.global_variables_initializer())\n","\n","  # Construct content_loss using content_image.\n","  sess.run(model['input'].assign(content_image))\n","  content_loss = content_loss_func(sess, model)\n","\n","  # Construct shape loss using content image\n","  sess.run(model[\"input\"].assign(initial_image))\n","  dist_template_inf, content_dist_sum = distance_transform.dist_t(content_image)\n","  ### take power of distance template\n","  dist_template = np.power(dist_template_inf,8)\n","  dist_template[dist_template>np.power(2,30)] = np.power(2,30)\n","\n","  shape_loss = shape_loss_func(sess, model, dist_template, content_dist_sum)\n","\n","  # Construct style_loss using style_image.\n","  sess.run(model['input'].assign(style_image))\n","  style_loss = style_loss_func(sess, model)\n","\n","  # Instantiate equation 7 of the paper.\n","  total_loss = alpha * content_loss + beta * style_loss + gamma * shape_loss\n","\n","  # Then we minimize the total_loss, which is the equation 7.\n","  optimizer = tf.train.AdamOptimizer(1.0)\n","  train_step = optimizer.minimize(total_loss)\n","\n","  sess.run(tf.global_variables_initializer())\n","  sess.run(model['input'].assign(initial_image))\n","  for it in range(ITERATIONS+1):\n","      sess.run(train_step)\n","      \n","      if it%100 == 0:\n","          # Print every 100 iteration.\n","          mixed_image = sess.run(model['input'])\n","          print('Iteration %d' % (it))\n","          print('sum         : ', sess.run(tf.reduce_sum(mixed_image)))\n","          print('total_loss  : ', sess.run(total_loss))\n","          print(\"content_loss: \", alpha*sess.run(content_loss))\n","          print(\"style_loss  : \", beta *sess.run(style_loss))\n","          print(\"shape loss  : \", gamma*sess.run(shape_loss))\n","\n","          if not os.path.exists(OUTPUT_DIR):\n","              os.mkdir(OUTPUT_DIR)\n","\n","          filename = OUTPUT_DIR + '/%d.jpg' % (it)\n","          utility.save_image(filename, mixed_image, invert = result_invert)\n","      if sess.run(total_loss) < 1:\n","          break\n","  sess.close()\n","  end_time = time.time()\n","  print(\"Time taken = \", end_time - start_time)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lCCt3-UUaQjr","executionInfo":{"status":"ok","timestamp":1649708392430,"user_tz":240,"elapsed":291673,"user":{"displayName":"Lichun Li","userId":"15395584703944545137"}},"outputId":"22434fe6-0d39-43cb-fe23-67298f0bd6c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["image resized to  (400, 400, 3)\n","Image file shape is:  (1, 400, 400, 3)\n","image resized to  (400, 400, 3)\n","Image file shape is:  (1, 400, 400, 3)\n","Iteration 0\n","sum         :  -8951308.0\n","total_loss  :  1.4075806e+20\n","content_loss:  1554129.92\n","style_loss  :  4551519232.0\n","shape loss  :  1.4075806980562508e+20\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 100\n","sum         :  -13048419.0\n","total_loss  :  1867224100000000.0\n","content_loss:  85704409.088\n","style_loss  :  1820552192.0\n","shape loss  :  1867222257248501.8\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 200\n","sum         :  -17154418.0\n","total_loss  :  40459340000.0\n","content_loss:  101421146.112\n","style_loss  :  1222464384.0\n","shape loss  :  39135459409.92\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 300\n","sum         :  -21012552.0\n","total_loss  :  63102440000.0\n","content_loss:  108817219.584\n","style_loss  :  922188608.0\n","shape loss  :  62071436738.56\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 400\n","sum         :  -24571092.0\n","total_loss  :  38781284000.0\n","content_loss:  113079345.152\n","style_loss  :  734735104.0\n","shape loss  :  37933471498.24\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 500\n","sum         :  -27730870.0\n","total_loss  :  36624994000.0\n","content_loss:  115635634.176\n","style_loss  :  609540288.0\n","shape loss  :  35899818639.36\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 600\n","sum         :  -30512576.0\n","total_loss  :  35894800000.0\n","content_loss:  117610209.28\n","style_loss  :  520763040.0\n","shape loss  :  35256428134.4\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 700\n","sum         :  -32955740.0\n","total_loss  :  105443330000.0\n","content_loss:  119343865.856\n","style_loss  :  457197760.0\n","shape loss  :  104866785525.76001\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 800\n","sum         :  -35100060.0\n","total_loss  :  114212100000.0\n","content_loss:  120711192.576\n","style_loss  :  409966400.0\n","shape loss  :  113681429954.56\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 900\n","sum         :  -36987584.0\n","total_loss  :  139434234000000.0\n","content_loss:  121775382.528\n","style_loss  :  373588672.0\n","shape loss  :  139433740295208.97\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 1000\n","sum         :  -38648224.0\n","total_loss  :  66319130000000.0\n","content_loss:  122724605.952\n","style_loss  :  344482112.0\n","shape loss  :  66318665179463.68\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 1100\n","sum         :  -40092092.0\n","total_loss  :  5.3955703e+16\n","content_loss:  123548033.024\n","style_loss  :  321360256.0\n","shape loss  :  5.395570189066568e+16\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 1200\n","sum         :  -41351296.0\n","total_loss  :  2536854000000000.0\n","content_loss:  124169109.50400001\n","style_loss  :  302688256.0\n","shape loss  :  2536853528945623.0\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 1300\n","sum         :  -42424424.0\n","total_loss  :  1.2980958e+17\n","content_loss:  124728958.976\n","style_loss  :  287710656.0\n","shape loss  :  1.2980958522337395e+17\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 1400\n","sum         :  -43339584.0\n","total_loss  :  1.8522508e+16\n","content_loss:  125241114.624\n","style_loss  :  275347840.0\n","shape loss  :  1.852250894607843e+16\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 1500\n","sum         :  -44121436.0\n","total_loss  :  2.3368344e+16\n","content_loss:  125692788.736\n","style_loss  :  264856912.0\n","shape loss  :  2.3368343937100024e+16\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 1600\n","sum         :  -44757970.0\n","total_loss  :  5.1296987e+17\n","content_loss:  126044209.15200001\n","style_loss  :  256465968.0\n","shape loss  :  5.129698931621495e+17\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 1700\n","sum         :  -45251204.0\n","total_loss  :  3.9266747e+16\n","content_loss:  126364721.15200001\n","style_loss  :  250661392.0\n","shape loss  :  3.926674881160151e+16\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 1800\n","sum         :  -45641824.0\n","total_loss  :  2.006789e+17\n","content_loss:  126583701.50400001\n","style_loss  :  245876592.0\n","shape loss  :  2.0067890405981552e+17\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 1900\n","sum         :  -45939504.0\n","total_loss  :  9.2388636e+17\n","content_loss:  126798782.464\n","style_loss  :  241749312.0\n","shape loss  :  9.238863715885489e+17\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 2000\n","sum         :  -46184450.0\n","total_loss  :  3.7957645e+16\n","content_loss:  126982135.808\n","style_loss  :  238772176.0\n","shape loss  :  3.795764827733885e+16\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 2100\n","sum         :  -46375372.0\n","total_loss  :  2470650100000000.0\n","content_loss:  127118426.112\n","style_loss  :  236213472.0\n","shape loss  :  2470649872252928.0\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 2200\n","sum         :  -46532800.0\n","total_loss  :  4106498800000000.0\n","content_loss:  127263760.384\n","style_loss  :  233890720.0\n","shape loss  :  4106498633493381.0\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 2300\n","sum         :  -46659096.0\n","total_loss  :  1887298700000000.0\n","content_loss:  127380733.952\n","style_loss  :  231861248.0\n","shape loss  :  1887298308779540.5\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 2400\n","sum         :  -46759064.0\n","total_loss  :  5544049000000000.0\n","content_loss:  127463342.08\n","style_loss  :  230127408.0\n","shape loss  :  5544048582878823.0\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 2500\n","sum         :  -46839156.0\n","total_loss  :  5226179000000000.0\n","content_loss:  127543009.28\n","style_loss  :  228548528.0\n","shape loss  :  5226178740496630.0\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 2600\n","sum         :  -46908570.0\n","total_loss  :  3.2796807e+17\n","content_loss:  127570001.92\n","style_loss  :  227120912.0\n","shape loss  :  3.279680897739548e+17\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 2700\n","sum         :  -46957504.0\n","total_loss  :  1073283340000000.0\n","content_loss:  127629778.944\n","style_loss  :  225682432.0\n","shape loss  :  1073283035593441.2\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 2800\n","sum         :  -46997744.0\n","total_loss  :  2.831307e+16\n","content_loss:  127666192.384\n","style_loss  :  224436976.0\n","shape loss  :  2.831307037831332e+16\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 2900\n","sum         :  -47033696.0\n","total_loss  :  4551426000000000.0\n","content_loss:  127705464.832\n","style_loss  :  223353024.0\n","shape loss  :  4551425634399683.0\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 3000\n","sum         :  -47060924.0\n","total_loss  :  1.3424938e+16\n","content_loss:  127735939.072\n","style_loss  :  222373568.0\n","shape loss  :  1.342493801909846e+16\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 3100\n","sum         :  -47084336.0\n","total_loss  :  1.2033438e+16\n","content_loss:  127756853.248\n","style_loss  :  221464928.0\n","shape loss  :  1.2033437334671196e+16\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 3200\n","sum         :  -47105276.0\n","total_loss  :  2008369700000000.0\n","content_loss:  127781961.728\n","style_loss  :  220464528.0\n","shape loss  :  2008369313685176.2\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 3300\n","sum         :  -47121240.0\n","total_loss  :  1.7195852e+17\n","content_loss:  127804817.408\n","style_loss  :  219614000.0\n","shape loss  :  1.719585278453298e+17\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 3400\n","sum         :  -47138716.0\n","total_loss  :  2778628800000000.0\n","content_loss:  127814402.04800001\n","style_loss  :  218823856.0\n","shape loss  :  2778628748549816.5\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 3500\n","sum         :  -47147456.0\n","total_loss  :  2.7648527e+17\n","content_loss:  127843696.64\n","style_loss  :  217911232.0\n","shape loss  :  2.7648528299264573e+17\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 3600\n","sum         :  -47154252.0\n","total_loss  :  5745854000000000.0\n","content_loss:  127843270.656\n","style_loss  :  217229888.0\n","shape loss  :  5745853634235597.0\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 3700\n","sum         :  -47153784.0\n","total_loss  :  2.8678143e+17\n","content_loss:  127860015.104\n","style_loss  :  216537216.0\n","shape loss  :  2.8678143972862854e+17\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 3800\n","sum         :  -47159340.0\n","total_loss  :  1.5004642e+18\n","content_loss:  127860473.856\n","style_loss  :  215900816.0\n","shape loss  :  1.5004642878510282e+18\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 3900\n","sum         :  -47159580.0\n","total_loss  :  1.6258162e+16\n","content_loss:  127863144.448\n","style_loss  :  215455792.0\n","shape loss  :  1.6258162330330726e+16\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 4000\n","sum         :  -47157770.0\n","total_loss  :  1.8506909e+16\n","content_loss:  127852527.616\n","style_loss  :  215005360.0\n","shape loss  :  1.8506908250469828e+16\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 4100\n","sum         :  -47157796.0\n","total_loss  :  7.876699e+16\n","content_loss:  127863259.136\n","style_loss  :  214532928.0\n","shape loss  :  7.876698991247754e+16\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 4200\n","sum         :  -47159870.0\n","total_loss  :  8201326500000000.0\n","content_loss:  127894085.632\n","style_loss  :  214075120.0\n","shape loss  :  8201325951057920.0\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 4300\n","sum         :  -47163510.0\n","total_loss  :  1.8811733e+16\n","content_loss:  127890063.36\n","style_loss  :  213612496.0\n","shape loss  :  1.8811734105375376e+16\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 4400\n","sum         :  -47161416.0\n","total_loss  :  7345077500000000.0\n","content_loss:  127906422.78400001\n","style_loss  :  213220512.0\n","shape loss  :  7345077147758756.0\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 4500\n","sum         :  -47163010.0\n","total_loss  :  1486584800000000.0\n","content_loss:  127911976.96000001\n","style_loss  :  212817104.0\n","shape loss  :  1486584388413030.5\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 4600\n","sum         :  -47163652.0\n","total_loss  :  4356413000000000.0\n","content_loss:  127925411.84\n","style_loss  :  212428656.0\n","shape loss  :  4356412816123494.5\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 4700\n","sum         :  -47165930.0\n","total_loss  :  5678763300000000.0\n","content_loss:  127942885.376\n","style_loss  :  212048624.0\n","shape loss  :  5678763152695624.0\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 4800\n","sum         :  -47166440.0\n","total_loss  :  7.337832e+16\n","content_loss:  127964831.744\n","style_loss  :  211644576.0\n","shape loss  :  7.337832190765434e+16\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 4900\n","sum         :  -47173570.0\n","total_loss  :  1.0325162e+18\n","content_loss:  127950258.176\n","style_loss  :  211448112.0\n","shape loss  :  1.0325161852723528e+18\n","Saved image file shape is:  (400, 400, 3)\n","Iteration 5000\n","sum         :  -47166660.0\n","total_loss  :  8149628500000000.0\n","content_loss:  127980388.352\n","style_loss  :  211015424.0\n","shape loss  :  8149628288709427.0\n","Saved image file shape is:  (400, 400, 3)\n","Time taken =  291.70522952079773\n"]}]}]}